# -*- coding: utf-8 -*-
"""Llama_3_8b_Unsloth.ipynb

Automatically generated by Colab.

Original file is located at
		https://colab.research.google.com/drive/1xKGYBMYEaSruQjsrGBqYfdZ3w9CRRlvQ

# Finetuning LLaMA-3 8B

task: ONR

optim: LoRa

Table of context:
1. Imports and Installs
2. Utils
3. Data Import
4. Model and Trainer Definisions
5. Training and Testing Process
"""
import sys

PARAMS = dict({
	'method': sys.argv[1], #TL or DA

	'train_on': [str(i) for i in sys.argv[2].split(',')],
	'test_on': [str(i) for i in sys.argv[3].split(',')],
	'DA_mixture': [str(i) for i in sys.argv[4].split(',')], # NB, SB, CB

	'epochs': 60,
	'LR': 0.5e-4,

	'Zero-Shot': int(sys.argv[5]),
	'Fine_Tune': int(sys.argv[6]),
	"save_dir": sys.argv[7]
	})

print(PARAMS)

"""## Installs and Imports"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install triton
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# !pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes
# !pip install trl bitsandbytes peft

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install xFormers

from tqdm import tqdm
import random
import os
from sklearn.metrics import mean_squared_error, r2_score
from collections import Counter
import numpy as np
import re
## model

from unsloth import FastLanguageModel
import torch

import pickle
from datasets import Dataset
## training libraries
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

"""## Utils"""

def extract_float(text):
		match = re.search(r"[-+]?\d*\.\d+", text)
		if match:
				return float(match.group(0))
		else:
				return None

def r44(value): return f"{round(value, 3):.3f}"

def r55(value): return f"{round(value, 4):.4f}"

def rrmse_calculator(targets, predictions):
		return np.sqrt(((predictions - targets) ** 2).mean())/np.mean(targets)

def metrics(y_test, y_pred, roundd = False):
		rrmse_ = rrmse_calculator(y_test, y_pred)
		rmse = mean_squared_error(y_test, y_pred) ** (1/2)
		r2 = r2_score(y_test, y_pred)
		return rrmse_, rmse, r2

def stid(source_name, target_name):
		return (source_name[0:3] + '-' + target_name[0:3]).lower()

def np_ratio(arr):
		C = Counter(arr)
		return C[0]/C[1]

def evaluator(test_data, model):
		generated_output = []
		real_output = []
		FastLanguageModel.for_inference(model) # Enable native 2x faster inference
		for testcases in tqdm(test_data):
				inputs = tokenizer(
				[
					alpaca_prompt.format(
							testcases["instruction"], # instruction
							testcases["input"], # input
							"", # output - leave this blank for generation!
					)
			], return_tensors = "pt").to("cuda")

				outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)

				outputs = tokenizer.batch_decode(outputs)[0]

				outputs = extract_float(outputs.split("### Response:")[1].strip())
				benchmark = extract_float(testcases["output"])

				generated_output.append(outputs)
				real_output.append(benchmark)

		valid_indices = [i for i, output in enumerate(generated_output) if output is not None]
		generated_output_filtered = np.array([generated_output[i] for i in valid_indices])
		real_output_filtered = np.array([real_output[i] for i in valid_indices])
		print('validated generations:', len(generated_output_filtered)/len(test_data))
		# print(generated_output_filtered)
		rrmse__, rmse, r2 = metrics(real_output_filtered, generated_output_filtered)
		# print(rrmse, rmse, r2)
		return rrmse__, rmse, r2

def translator(i):
	if i == "SB":
		return "Southern Chicago"
	elif i == "CB":
		return "Central Chicago"
	elif i == "NB":
		return "Northern Chicago"

def get_dataset(trains, add_loc = False):
	datas = []
	for i in trains:
		with open('./serialized_'+i+'_train.pkl', 'rb') as f:
			data = pickle.load(f)
			if add_loc:
				for j in data:
					j['input'] = j['input'] + 'Location: ' + translator(i)
			datas = datas+data

	return datas, Dataset.from_dict({key: [d[key] for d in datas] for key in datas[0]})

def get_test_dataset(trains, add_loc = False):
	datas = []
	for i in trains:
		with open('./serialized_'+i+'_test.pkl', 'rb') as f:
			data = pickle.load(f)
			if add_loc:
				for j in data:
					j['input'] = j['input'] + 'Location: ' + translator(i)
			datas = datas+data

	return datas, Dataset.from_dict({key: [d[key] for d in datas] for key in datas[0]})

def get_dataset_DA(sources, targets, num):
	s, _ = get_dataset(sources, True)
	t, _ = get_dataset(targets, True)
	num = int(len(t) * 0.12)
	random_indices = random.sample(range(len(t)), num)

	for idx in random_indices:
		s.append(t[idx])
	random.shuffle(s)
	datas = s
	return datas, Dataset.from_dict({key: [d[key] for d in datas] for key in datas[0]})

results = []
def logger(source, target, rrmse_val, rmse, r2, model = 'LLaMA3-8B'):
		results.append({
				'model': model,
				'source': source,
				'target': target,
				'rrmse': rrmse_val,
				'rmse': rmse,
				'r2': r2,
		})

"""## Model"""

max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
		"unsloth/mistral-7b-v0.3-bnb-4bit",      # New Mistral v3 2x faster!
		"unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
		"unsloth/llama-3-8b-bnb-4bit",           # Llama-3 15 trillion tokens model 2x faster!
		"unsloth/llama-3-8b-Instruct-bnb-4bit",
		"unsloth/llama-3-70b-bnb-4bit",
		"unsloth/Phi-3-mini-4k-instruct",        # Phi-3 2x faster!
		"unsloth/Phi-3-medium-4k-instruct",
		"unsloth/mistral-7b-bnb-4bit",
		"unsloth/gemma-7b-bnb-4bit",             # Gemma 2.2x faster!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
		model_name = "unsloth/llama-3-8b-bnb-4bit",
		max_seq_length = max_seq_length,
		dtype = dtype,
		load_in_4bit = load_in_4bit,
		# token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)

model = FastLanguageModel.get_peft_model(
		model,
		r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
		target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
											"gate_proj", "up_proj", "down_proj",],
		lora_alpha = 16,
		lora_dropout = 0, # Supports any, but = 0 is optimized
		bias = "none",    # Supports any, but = "none" is optimized
		# [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
		use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
		random_state = 3407,
		use_rslora = False,  # We support rank stabilized LoRA
		loftq_config = None, # And LoftQ
)

"""## Data Import"""

alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context.\
									 Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{} Is the water [safe] or [unsafe], select one based on the information.

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
		instructions = examples["instruction"]
		inputs       = examples["input"]
		outputs      = examples["output"]
		texts = []
		for instruction, input, output in zip(instructions, inputs, outputs):
				# Must add EOS_TOKEN, otherwise your generation will go on forever!
				text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
				texts.append(text)
		return { "text" : texts, }
pass

if PARAMS['method'] == 'TL':
	print('TL train data')
	train_dict, train_dataset = get_dataset(PARAMS['train_on'])
	dataset = train_dataset.map(formatting_prompts_func, batched = True,)
	print('train data', PARAMS['train_on'], len(train_dict))

if PARAMS['method'] == 'DA':
	print('DA train data')
	train_dict, train_dataset = get_dataset_DA(PARAMS['train_on'], PARAMS['DA_mixture'], 500)
	dataset = train_dataset.map(formatting_prompts_func, batched = True,)
	print('train data', PARAMS['train_on'], len(train_dict))

test_dict = {}
for i in PARAMS['test_on']:
	if PARAMS['method'] == 'DA':
		test_dict_, test_dataset = get_test_dataset([i], add_loc=True)
	else:
		test_dict_, test_dataset = get_test_dataset([i])
	test_dict[i] = test_dict_

	print(i, 'test data', len(test_dict_))

"""## Train and Evaluate"""

if PARAMS['Zero-Shot']:
	print('testing zero-shot model ...')
	for i in test_dict.keys():
		try:
			rrmse_val, rmse, r2 = evaluator(test_dict[i], model)
			logger('Zero-Shot', i, rrmse_val, rmse, r2, model = 'LLaMA3-8B')
			print(results[-1])
		except:
			print('error: Zero-Shot test on:', i, 'error occured.')

trainer = SFTTrainer(
		model = model,
		tokenizer = tokenizer,
		train_dataset = dataset,
		dataset_text_field = "text",
		max_seq_length = max_seq_length,
		dataset_num_proc = 2,
		packing = False, # Can make training 5x faster for short sequences.
		args = TrainingArguments(
				per_device_train_batch_size = 2,
				gradient_accumulation_steps = 4,
				warmup_steps = 5,
				max_steps = PARAMS['epochs'],
				learning_rate = PARAMS['LR'],
				fp16 = not is_bfloat16_supported(),
				bf16 = is_bfloat16_supported(),
				logging_steps = 1,
				optim = "adamw_8bit",
				weight_decay = 0.02,
				lr_scheduler_type = "linear",
				seed = 3407,
				output_dir = "outputs",
		),
)

if PARAMS['Fine_Tune']:
	trainer_stats = trainer.train()

if PARAMS['Fine_Tune']:
	print('testing fine-tuned model ...')
	for i in test_dict.keys():
		try:
			rrmseee, rmse, r2 = evaluator(test_dict[i], model)
			logger(",".join(PARAMS['train_on']), i, rrmseee, rmse, r2, model = 'LLaMA3-8B')
			print(results[-1])
		except:
			print('error: fine-tuned on:', ",".join(PARAMS['train_on']), 'test on:', i, 'error occured.')

for i in results:
	print(i)
if len(results) == 0:
	print('Nothing has been done.')

with open(PARAMS['save_dir'], 'w') as file:
    for item in results:
        file.write(str(item) + '\n')











